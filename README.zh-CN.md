Here is the Chinese translation of your README:

---

# KnowledgeUpdate

[中文版本](README.zh-CN.md) | [English Version](README.md)

通过 LoRA 微调在一天内更新大语言模型 (LLM) 知识项目仓库。

## 目录

- [环境设置](#环境设置)
- [数据收集](#数据收集)
- [知识更新与微调](#知识更新与微调)

  - [概述](#概述)
  - [微调流程](#微调流程)
  - [运行微调脚本](#运行微调脚本)

- [测试用例 1](#测试用例-1)
  - [目标](#目标)
  - [测试环境](#测试环境)
  - [测试流程](#测试流程)
  - [通过/失败标准](#通过失败标准)
  - [执行命令](#执行命令)
- [测试用例 2](#测试用例-2)
  - [目标](#目标-1)
  - [测试环境](#测试环境-1)
  - [测试流程](#测试流程-1)
  - [执行步骤](#执行步骤)
  - [通过/失败标准](#通过失败标准-1)
  - [输出](#输出)

---

## 环境设置

在Conda 环境中运行以下命令：

```bash
pip install -r requirements.txt
```

---

## 数据收集

数据收集过程将文本文件自动处理为适用于 LLM 监督微调 (SFT) 的 `.jsonl` 格式的数据集。该脚本会遍历指定目录中的所有文本文件，提取事实，并根据每个事实生成带有多样化角色的 Q&A 对。

数据收集方法基于[这篇论文](https://arxiv.org/abs/2404.00213)中的事实抽取方法。要运行数据收集过程，请使用 `collect_data.sh` 脚本。此脚本会启动 `collect_data.py`，并传递所有必要参数。处理步骤如下：

1. **文本分块**：脚本根据 `chunk_size_by_token` 参数对每个 `.txt` 文件进行分词和分块。
2. **主题摘要**：对每个分块的主题进行总结，以便于后续的事实抽取。
3. **事实抽取**：从文本内容中提取独立的事实。
4. **Q&A 生成**: 根据每个事实生成 Q&A 对。
5. **基于角色的多样化**：为每个事实分配角色，以多样化生成的 Q&A 对。

### 配置选项

上述过程可通过以下 `collect_data.sh` 中的参数进行配置：

- **`--data_path`**：输入文件所在的目录路径。
- **`--file_type`**：输入文件类型，支持 `'docx'` 或 `'txt'`。
- **`--model_name_or_path`**：预训练模型路径或 Hugging Face 模型 ID。
- **`--model_type`**：模型后端，支持 `'vllm'`、`'hf'` 或 `'api'`。
  - 设置为 `'api'` 时，确保正确配置 `utils/helper.py` 中的 `api_chat` 函数。
- **`--chunk_size_by_token`**：文本分块的 token 限制；值越小，提取的内容越精细，但处理速度更慢。
- **`--qa_amount_per_fact`**：每个事实生成的 Q&A 对数量。
- **`--role_amount_per_fact`**：每个事实分配的角色数量，用于多样化 Q&A。
- **`--save_dir`**：保存生成 `.jsonl` 数据集的文件路径（默认：`datasets/qa_pairs.jsonl`）。

### 启动数据收集

使用默认设置启动数据收集，请运行：

```bash
bash collect_data.sh
```

---

## 知识更新与微调

### 概述

微调过程通过对预训练模型进行定制，以提高其在特定数据集上的性能。此设置支持使用 **DeepSpeed** 进行多 GPU 微调。我们使用 **LoRA**（低秩适配）技术，以更高效的方式完成模型训练，减少额外参数需求。整个过程集成了 **wandb**（用于实验跟踪）和 **DeepSpeed**（用于高效的内存与计算资源管理）。

### 微调流程

`finetune.py` 脚本基于 Hugging Face Transformers 库，并使用了 `deepspeed` 和 `peft` 等库以支持 LoRA 微调。主要功能包括：

1. **参数解析**：
   - 定义模型、数据、训练和 LoRA 参数的配置。
2. **数据预处理**：
   - 支持预加载或延迟加载两种数据处理模式，提供灵活的内存使用。
3. **训练数据集**：
   - 支持预加载数据集 `SupervisedDataset` 和延迟加载数据集 `LazySupervisedDataset`，适合处理更大数据集。
4. **LoRA 配置**：
   - 如果启用 `use_lora`，脚本会应用 LoRA 配置以降低内存需求。
5. **训练**：
   - 使用 Hugging Face 的 `Trainer` 类设置分布式训练，并支持 wandb 无缝集成。
6. **保存**：
   - 训练结束后，使用 `safe_save_model_for_hf_trainer` 函数保存模型状态。

---

### 运行微调脚本

通过 `shells` 文件夹中的 `finetune_lora_ds.sh` 脚本启动微调。主要步骤如下：

1. **多 GPU 配置**：

   - `GPUS_PER_NODE`：每节点 GPU 数量。
   - `NNODES`：GPU 节点数量。
   - `NODE_RANK`：当前节点的编号。
   - `MASTER_ADDR`：主节点 IP 地址。
   - `MASTER_PORT`：通信端口。

2. **更新模型和数据路径**：

   - 设置 `MODEL` 为模型路径。
   - 设置 `DATA` 为数据集路径。

3. **使用可选参数运行**：

   - `DS_CONFIG_PATH`：DeepSpeed 配置文件路径。
   - `WANDB_KEY`：wandb API 密钥。

运行以下命令启动脚本：

```bash
bash shells/finetune_lora_ds.sh
```

**注意：启动微调前，请调整 `per_device_train_batch_size` 参数，以找到适合资源设置的最佳批量大小。**

## 测试用例 1

### **目标**
测试使用生成的数据集进行 LoRA 微调是否可在指定硬件设置下 **8 小时内完成**。测试包括运行三个微调实验，并计算总运行时间的平均值以评估时间效率。

### **测试环境**
- **硬件配置**：
  - 使用的 GPU：`CUDA_VISIBLE_DEVICES=x,x,x,x`（4 块 A100 GPU）
  - `GPUS_PER_NODE` 参数由脚本动态确定。

- **数据集**：
  - 三个数据集分别为 `qa_pairs_001.jsonl`、`qa_pairs_002.jsonl` 和 `qa_pairs_003.jsonl`。
  - 每次运行分别处理一个数据集。

- **微调脚本**：
  - 脚本通过 `torchrun` 启动，用于分布式多 GPU 训练。
  - 关键参数：
    - 每 GPU 批量大小：`32`
    - 梯度累积步数：`4`
    - 训练轮次：`5`
    - 学习率：`3e-4`
    - 延迟预处理：启用
    - LoRA 微调：启用

### **测试流程**
1. **执行**：
   - 测试运行三次微调过程，每次使用一个指定数据集，并将输出模型保存到单独的目录中。
   - 记录每次运行的耗时。

2. **时间测量**：
   - 记录每次运行的开始时间和结束时间。
   - 总运行时间为三次运行耗时的总和。
   - 计算平均运行时间。

3. **输出**：
   - 脚本会在所有运行完成后打印平均运行时间（以小时为单位）。

### **通过/失败标准**
- **通过**：三次运行的平均时间 **≤ 8 小时**。
- **失败**：三次运行的平均时间超过 8 小时。

### **执行命令**
运行以下脚本开始测试：
```bash
bash test_case_1.sh
```

---

## 测试用例 2

### **目标**
验证 **测试用例 1** 中的微调损失是否收敛，通过 **柯西准则测试** 检查损失值的差异是否小于指定的阈值。

### **测试环境**
- **输入**：
  - 来自三次微调运行的 `.npy` 文件，包含损失值，存储在 `losses` 文件夹中。
- **阈值**：
  - 默认收敛阈值为 **0.001**。
  - 可在运行脚本时自定义该值。

### **测试流程**
1. **脚本执行**：
   - 使用 Python 脚本 `cauchy_test.py` 执行测试，可通过 `bash cauchy_test.sh` 启动。
   - 对指定 `losses` 文件夹中的每个 `.npy` 文件应用柯西测试。

2. **收敛检查**：
   - 对每个损失文件，检查连续损失值的绝对差是否低于阈值。
   - 如果所有文件通过柯西测试，则损失被认为收敛。

3. **输出**：
   - 脚本输出一张表格，显示每个损失文件是否通过收敛测试。

### **执行步骤**
1. **运行 Bash 脚本**：
   使用提供的 `cauchy_test.sh` 脚本启动 Python 收敛测试：
   ```bash
   bash cauchy_test.sh
   ```
   - **选项**：
     - `--folder` 或 `-f`：包含 `.npy` 文件的文件夹路径（默认：`losses`）。
     - `--threshold` 或 `-t`：柯西准则的收敛阈值（默认：`0.001`）。
     - `--script` 或 `-s`：Python 脚本路径（默认：`cauchy_test.py`）。

   示例自定义命令：
   ```bash
   bash cauchy_test.sh -f custom_losses -t 0.0005
   ```

2. **直接运行 Python 脚本**：
   或者，您可以直接执行 Python 脚本：
   ```bash
   python cauchy_test.py --loss_folder losses --threshold 0.001
   ```

### **通过/失败标准**
- **通过**：
  - 三个 `.npy` 损失文件全部通过柯西测试，表示损失收敛。
- **失败**：
  - 任意一个 `.npy` 文件未通过柯西测试，表示损失未收敛。

### **输出**
运行脚本后，将显示以下结果表：

| **文件名**          | **收敛？** |
|---------------------|------------|
| `qa_pairs_001.npy`  | ✅ 是      |
| `qa_pairs_002.npy`  | ✅ 是      |
| `qa_pairs_003.npy`  | ✅ 是      |

如果所有文件均通过测试，则测试通过；否则测试失败。